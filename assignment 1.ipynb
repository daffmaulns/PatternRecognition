{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Preprocessing dan Ekstraksi Fitur TF-IDF\n",
        "\n",
        "---\n",
        "\n",
        "### Konsep yang Digunakan:\n",
        "- **TF (Term Frequency)**: `TF(t,d) = jumlah kemunculan kata t / total kata dalam dokumen d`\n",
        "- **IDF (Inverse Document Frequency)**: `IDF(t) = log(N / df(t))`\n",
        "- **TF-IDF**: `TF-IDF(t,d) = TF(t,d) × IDF(t)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section1"
      },
      "source": [
        "## 1. Fungsi Helper (Tanpa Library)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper_functions"
      },
      "outputs": [],
      "source": [
        "def hitung_log(x):\n",
        "    \"\"\"\n",
        "    Menghitung logaritma natural menggunakan ekspansi Taylor Series\n",
        "    ln(x) = (x-1) - (x-1)^2/2 + (x-1)^3/3 - ...\n",
        "    \"\"\"\n",
        "    if x <= 0:\n",
        "        return float('-inf')\n",
        "\n",
        "    # Normalisasi x ke range [0.5, 1.5] untuk akurasi lebih baik\n",
        "    exp_adjust = 0\n",
        "    while x > 1.5:\n",
        "        x = x / 2.718281828459045  # e\n",
        "        exp_adjust += 1\n",
        "    while x < 0.5:\n",
        "        x = x * 2.718281828459045\n",
        "        exp_adjust -= 1\n",
        "\n",
        "    # Ekspansi Taylor\n",
        "    result = 0\n",
        "    z = x - 1\n",
        "    term = z\n",
        "\n",
        "    for n in range(1, 100):\n",
        "        result += term / n\n",
        "        term *= -z\n",
        "        if abs(term / n) < 0.0000001:  # Konvergensi\n",
        "            break\n",
        "\n",
        "    return result + exp_adjust\n",
        "\n",
        "\n",
        "def adalah_huruf(char):\n",
        "    \"\"\"Cek apakah karakter adalah huruf\"\"\"\n",
        "    return ('a' <= char <= 'z') or ('A' <= char <= 'Z')\n",
        "\n",
        "\n",
        "def adalah_angka(char):\n",
        "    \"\"\"Cek apakah karakter adalah angka\"\"\"\n",
        "    return '0' <= char <= '9'\n",
        "\n",
        "\n",
        "def adalah_whitespace(char):\n",
        "    \"\"\"Cek apakah karakter adalah whitespace\"\"\"\n",
        "    return char in ' \\t\\n\\r'\n",
        "\n",
        "\n",
        "def ke_huruf_kecil(char):\n",
        "    \"\"\"Ubah karakter ke huruf kecil\"\"\"\n",
        "    if 'A' <= char <= 'Z':\n",
        "        return chr(ord(char) + 32)\n",
        "    return char\n",
        "\n",
        "\n",
        "def cetak_garis(panjang=70, karakter=\"=\"):\n",
        "    \"\"\"Mencetak garis pemisah\"\"\"\n",
        "    for i in range(panjang):\n",
        "        print(karakter, end=\"\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def format_angka(angka, desimal=4):\n",
        "    \"\"\"Format angka dengan jumlah desimal tertentu\"\"\"\n",
        "    format_str = \"{:.\" + str(desimal) + \"f}\"\n",
        "    return format_str.format(angka)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section2"
      },
      "source": [
        "## 2. Class TextPreprocessor\n",
        "\n",
        "Class ini melakukan preprocessing teks dengan tahapan:\n",
        "1. Lowercase (ubah ke huruf kecil)\n",
        "2. Remove punctuation (hapus tanda baca)\n",
        "3. Remove numbers (hapus angka)\n",
        "4. Tokenization (pecah menjadi kata)\n",
        "5. Remove stopwords (hapus kata-kata umum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocessor_class",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e537b3a8-e459-4787-e825-e7161db975e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Class TextPreprocessor berhasil didefinisikan!\n",
            "\n",
            "Contoh penggunaan:\n",
            "  Input : Python adalah bahasa pemrograman yang populer!\n",
            "  Output: ['python', 'bahasa', 'pemrograman', 'populer']\n"
          ]
        }
      ],
      "source": [
        "class TextPreprocessor:\n",
        "    \"\"\"Kelas untuk preprocessing teks\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Daftar stopwords bahasa Indonesia\n",
        "        self.stopwords = [\n",
        "            'yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia',\n",
        "            'dua', 'ia', 'seperti', 'jika', 'sehingga', 'kembali', 'dan', 'ini',\n",
        "            'itu', 'dalam', 'tidak', 'adalah', 'sebagai', 'lebih', 'akan', 'telah',\n",
        "            'dari', 'oleh', 'di', 'dengan', 'atau', 'ada', 'juga', 'bahwa', 'tersebut',\n",
        "            'sangat', 'dapat', 'sudah', 'bisa', 'karena', 'hanya', 'masih', 'yaitu'\n",
        "        ]\n",
        "\n",
        "    def lowercase(self, text):\n",
        "        \"\"\"Mengubah teks menjadi huruf kecil\"\"\"\n",
        "        result = \"\"\n",
        "        for char in text:\n",
        "            result += ke_huruf_kecil(char)\n",
        "        return result\n",
        "\n",
        "    def remove_punctuation_and_numbers(self, text):\n",
        "        \"\"\"Menghapus tanda baca dan angka\"\"\"\n",
        "        result = \"\"\n",
        "        for char in text:\n",
        "            if adalah_huruf(char) or adalah_whitespace(char):\n",
        "                result += char\n",
        "            elif not adalah_angka(char):\n",
        "                result += ' '  # Ganti tanda baca dengan spasi\n",
        "        return result\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Memecah teks menjadi token/kata\"\"\"\n",
        "        tokens = []\n",
        "        current_token = \"\"\n",
        "\n",
        "        for char in text:\n",
        "            if adalah_whitespace(char):\n",
        "                if current_token:\n",
        "                    tokens.append(current_token)\n",
        "                    current_token = \"\"\n",
        "            else:\n",
        "                current_token += char\n",
        "\n",
        "        # Tambahkan token terakhir\n",
        "        if current_token:\n",
        "            tokens.append(current_token)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        \"\"\"Menghapus stopwords\"\"\"\n",
        "        result = []\n",
        "        for token in tokens:\n",
        "            # Cek panjang token dan apakah stopword\n",
        "            is_stopword = False\n",
        "            for sw in self.stopwords:\n",
        "                if token == sw:\n",
        "                    is_stopword = True\n",
        "                    break\n",
        "\n",
        "            if not is_stopword and len(token) > 2:\n",
        "                result.append(token)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Pipeline lengkap preprocessing\"\"\"\n",
        "        text = self.lowercase(text)\n",
        "        text = self.remove_punctuation_and_numbers(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        tokens = self.remove_stopwords(tokens)\n",
        "        return tokens\n",
        "\n",
        "\n",
        "print(\"✓ Class TextPreprocessor berhasil didefinisikan!\")\n",
        "print(\"\\nContoh penggunaan:\")\n",
        "preprocessor = TextPreprocessor()\n",
        "contoh_teks = \"Python adalah bahasa pemrograman yang populer!\"\n",
        "hasil = preprocessor.preprocess(contoh_teks)\n",
        "print(f\"  Input : {contoh_teks}\")\n",
        "print(f\"  Output: {hasil}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section3"
      },
      "source": [
        "## 3. Class TFIDFExtractor\n",
        "\n",
        "Class ini menghitung TF-IDF untuk setiap dokumen dengan langkah:\n",
        "1. **Fit**: Membangun vocabulary dan menghitung IDF\n",
        "2. **Transform**: Mengubah dokumen menjadi vektor TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfidf_class"
      },
      "outputs": [],
      "source": [
        "class TFIDFExtractor:\n",
        "    \"\"\"Kelas untuk menghitung TF-IDF tanpa library\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vocabulary = []\n",
        "        self.idf_values = {}\n",
        "        self.documents = []\n",
        "\n",
        "    def hitung_kata(self, tokens):\n",
        "        \"\"\"Menghitung frekuensi setiap kata dalam list tokens\"\"\"\n",
        "        count = {}\n",
        "        for token in tokens:\n",
        "            if token in count:\n",
        "                count[token] += 1\n",
        "            else:\n",
        "                count[token] = 1\n",
        "        return count\n",
        "\n",
        "    def kata_unik(self, tokens):\n",
        "        \"\"\"Mendapatkan kata-kata unik dari list tokens\"\"\"\n",
        "        unique = []\n",
        "        for token in tokens:\n",
        "            if token not in unique:\n",
        "                unique.append(token)\n",
        "        return unique\n",
        "\n",
        "    def urutkan_list(self, items):\n",
        "        \"\"\"Mengurutkan list menggunakan bubble sort\"\"\"\n",
        "        n = len(items)\n",
        "        for i in range(n):\n",
        "            for j in range(0, n-i-1):\n",
        "                if items[j] > items[j+1]:\n",
        "                    items[j], items[j+1] = items[j+1], items[j]\n",
        "        return items\n",
        "\n",
        "    def fit(self, documents):\n",
        "        \"\"\"Menghitung IDF dari koleksi dokumen\"\"\"\n",
        "        self.documents = documents\n",
        "        N = len(documents)  # Jumlah dokumen\n",
        "\n",
        "        # Bangun vocabulary\n",
        "        all_words = []\n",
        "        for doc in documents:\n",
        "            for word in doc:\n",
        "                if word not in all_words:\n",
        "                    all_words.append(word)\n",
        "\n",
        "        self.vocabulary = self.urutkan_list(all_words)\n",
        "\n",
        "        # Hitung document frequency untuk setiap kata\n",
        "        df = {}\n",
        "        for word in self.vocabulary:\n",
        "            df[word] = 0\n",
        "\n",
        "        for doc in documents:\n",
        "            unique_words = self.kata_unik(doc)\n",
        "            for word in unique_words:\n",
        "                if word in df:\n",
        "                    df[word] += 1\n",
        "\n",
        "        # Hitung IDF\n",
        "        for word in self.vocabulary:\n",
        "            # IDF = log(N / df(t))\n",
        "            self.idf_values[word] = hitung_log(N / df[word])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def calculate_tf(self, tokens):\n",
        "        \"\"\"Menghitung Term Frequency (TF)\"\"\"\n",
        "        tf_count = self.hitung_kata(tokens)\n",
        "        total_words = len(tokens)\n",
        "\n",
        "        # Normalisasi TF\n",
        "        tf = {}\n",
        "        if total_words > 0:\n",
        "            for word in tf_count:\n",
        "                tf[word] = tf_count[word] / total_words\n",
        "\n",
        "        return tf\n",
        "\n",
        "    def transform(self, documents):\n",
        "        \"\"\"Mengubah dokumen menjadi vektor TF-IDF\"\"\"\n",
        "        tfidf_vectors = []\n",
        "\n",
        "        for doc in documents:\n",
        "            # Hitung TF\n",
        "            tf = self.calculate_tf(doc)\n",
        "\n",
        "            # Hitung TF-IDF\n",
        "            tfidf = {}\n",
        "            for word in self.vocabulary:\n",
        "                if word in tf:\n",
        "                    tfidf[word] = tf[word] * self.idf_values[word]\n",
        "                else:\n",
        "                    tfidf[word] = 0.0\n",
        "\n",
        "            tfidf_vectors.append(tfidf)\n",
        "\n",
        "        return tfidf_vectors\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        \"\"\"Fit dan transform dalam satu langkah\"\"\"\n",
        "        self.fit(documents)\n",
        "        return self.transform(documents)\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        \"\"\"Mendapatkan nama fitur (vocabulary)\"\"\"\n",
        "        return self.vocabulary\n",
        "\n",
        "    def vector_to_list(self, tfidf_vector):\n",
        "        \"\"\"Mengubah TF-IDF vector (dict) menjadi list\"\"\"\n",
        "        result = []\n",
        "        for word in self.vocabulary:\n",
        "            result.append(tfidf_vector[word])\n",
        "        return result\n",
        "\n",
        "    def urutkan_tfidf(self, tfidf_vector):\n",
        "        \"\"\"Mengurutkan kata berdasarkan nilai TF-IDF (descending)\"\"\"\n",
        "        items = []\n",
        "        for word in tfidf_vector:\n",
        "            items.append((word, tfidf_vector[word]))\n",
        "\n",
        "        # Bubble sort descending berdasarkan nilai\n",
        "        n = len(items)\n",
        "        for i in range(n):\n",
        "            for j in range(0, n-i-1):\n",
        "                if items[j][1] < items[j+1][1]:\n",
        "                    items[j], items[j+1] = items[j+1], items[j]\n",
        "\n",
        "        return items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4"
      },
      "source": [
        "## 4. Persiapan Data\n",
        "\n",
        "Siapkan dokumen teks yang akan diproses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffca8e5b-3da8-41e3-a72f-f8c314f5a3ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jumlah dokumen: 5\n",
            "\n",
            "Dokumen-dokumen:\n",
            "  1. Python adalah bahasa pemrograman yang populer untuk machine learning\n",
            "  2. Machine learning membutuhkan data yang banyak untuk pelatihan model\n",
            "  3. Bahasa pemrograman Python mudah dipelajari oleh pemula\n",
            "  4. Data science menggunakan Python dan R sebagai bahasa pemrograman utama\n",
            "  5. Model machine learning dapat digunakan untuk prediksi data\n"
          ]
        }
      ],
      "source": [
        "documents_text = [\n",
        "    \"Python adalah bahasa pemrograman yang populer untuk machine learning\",\n",
        "    \"Machine learning membutuhkan data yang banyak untuk pelatihan model\",\n",
        "    \"Bahasa pemrograman Python mudah dipelajari oleh pemula\",\n",
        "    \"Data science menggunakan Python dan R sebagai bahasa pemrograman utama\",\n",
        "    \"Model machine learning dapat digunakan untuk prediksi data\"\n",
        "]\n",
        "\n",
        "print(f\"\\nJumlah dokumen: {len(documents_text)}\")\n",
        "print(\"\\nDokumen-dokumen:\")\n",
        "for i, doc in enumerate(documents_text, 1):\n",
        "    print(f\"  {i}. {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section5"
      },
      "source": [
        "## 5. Preprocessing Teks\n",
        "\n",
        "Lakukan preprocessing pada setiap dokumen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocessing",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010ba991-c7de-45bd-e58b-d3baa4c756f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dokumen 1:\n",
            "  Original: Python adalah bahasa pemrograman yang populer untuk machine learning\n",
            "  Tokens  : ['python', 'bahasa', 'pemrograman', 'populer', 'machine', 'learning']\n",
            "  Jumlah token: 6\n",
            "\n",
            "Dokumen 2:\n",
            "  Original: Machine learning membutuhkan data yang banyak untuk pelatihan model\n",
            "  Tokens  : ['machine', 'learning', 'membutuhkan', 'data', 'banyak', 'pelatihan', 'model']\n",
            "  Jumlah token: 7\n",
            "\n",
            "Dokumen 3:\n",
            "  Original: Bahasa pemrograman Python mudah dipelajari oleh pemula\n",
            "  Tokens  : ['bahasa', 'pemrograman', 'python', 'mudah', 'dipelajari', 'pemula']\n",
            "  Jumlah token: 6\n",
            "\n",
            "Dokumen 4:\n",
            "  Original: Data science menggunakan Python dan R sebagai bahasa pemrograman utama\n",
            "  Tokens  : ['data', 'science', 'menggunakan', 'python', 'bahasa', 'pemrograman', 'utama']\n",
            "  Jumlah token: 7\n",
            "\n",
            "Dokumen 5:\n",
            "  Original: Model machine learning dapat digunakan untuk prediksi data\n",
            "  Tokens  : ['model', 'machine', 'learning', 'digunakan', 'prediksi', 'data']\n",
            "  Jumlah token: 6\n"
          ]
        }
      ],
      "source": [
        "preprocessor = TextPreprocessor()\n",
        "processed_docs = []\n",
        "\n",
        "for i in range(len(documents_text)):\n",
        "    doc = documents_text[i]\n",
        "    tokens = preprocessor.preprocess(doc)\n",
        "    processed_docs.append(tokens)\n",
        "\n",
        "    print(f\"\\nDokumen {i + 1}:\")\n",
        "    print(f\"  Original: {doc}\")\n",
        "    print(f\"  Tokens  : {tokens}\")\n",
        "    print(f\"  Jumlah token: {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6"
      },
      "source": [
        "## 6. Ekstraksi Fitur TF-IDF\n",
        "\n",
        "Hitung TF-IDF untuk setiap dokumen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfidf_extraction",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6bf256-49a5-46e4-d13c-ce319d59e44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah fitur (vocabulary): 19\n"
          ]
        }
      ],
      "source": [
        "tfidf_extractor = TFIDFExtractor()\n",
        "tfidf_vectors = tfidf_extractor.fit_transform(processed_docs)\n",
        "\n",
        "print(f\"Jumlah fitur (vocabulary): {len(tfidf_extractor.get_feature_names())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section7"
      },
      "source": [
        "## 7. Vocabulary (Fitur)\n",
        "\n",
        "Tampilkan semua kata unik yang menjadi fitur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vocabulary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6052197c-2841-44ec-fb72-69c603898e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jumlah fitur unik: 19\n",
            "\n",
            "Daftar fitur: ['bahasa', 'banyak', 'data', 'digunakan', 'dipelajari', 'learning', 'machine', 'membutuhkan', 'menggunakan', 'model', 'mudah', 'pelatihan', 'pemrograman', 'pemula', 'populer', 'prediksi', 'python', 'science', 'utama']\n"
          ]
        }
      ],
      "source": [
        "vocabulary = tfidf_extractor.get_feature_names()\n",
        "print(f\"\\nJumlah fitur unik: {len(vocabulary)}\")\n",
        "print(f\"\\nDaftar fitur: {vocabulary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section8"
      },
      "source": [
        "## 8. Nilai IDF\n",
        "\n",
        "Tampilkan nilai IDF untuk setiap kata dalam vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idf_values",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "119cd806-072a-477a-9855-068408726746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Kata                  IDF\n",
            "-----------------------------------\n",
            "bahasa              : 0.5108\n",
            "banyak              : 1.6094\n",
            "data                : 0.5108\n",
            "digunakan           : 1.6094\n",
            "dipelajari          : 1.6094\n",
            "learning            : 0.5108\n",
            "machine             : 0.5108\n",
            "membutuhkan         : 1.6094\n",
            "menggunakan         : 1.6094\n",
            "model               : 0.9163\n",
            "mudah               : 1.6094\n",
            "pelatihan           : 1.6094\n",
            "pemrograman         : 0.5108\n",
            "pemula              : 1.6094\n",
            "populer             : 1.6094\n",
            "prediksi            : 1.6094\n",
            "python              : 0.5108\n",
            "science             : 1.6094\n",
            "utama               : 1.6094\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nKata                  IDF\")\n",
        "print(\"-\" * 35)\n",
        "for word in vocabulary:\n",
        "    idf_val = tfidf_extractor.idf_values[word]\n",
        "    padding = \" \" * (20 - len(word))\n",
        "    print(f\"{word}{padding}: {format_angka(idf_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semakin tinggi nilai IDF, semakin jarang kata tersebut muncul di dokumen"
      ],
      "metadata": {
        "id": "D1-ScV87qjXI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section9"
      },
      "source": [
        "## 9. Vektor TF-IDF untuk Setiap Dokumen\n",
        "\n",
        "Tampilkan nilai TF-IDF tertinggi untuk setiap dokumen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfidf_vectors",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76660fd0-095d-4c76-ca6a-cbe5b45df467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dokumen 1: python bahasa pemrograman populer machine learning \n",
            "----------------------------------------------------------------------\n",
            "Kata (TF-IDF > 0):\n",
            "\n",
            "Kata                  TF-IDF\n",
            "-----------------------------------\n",
            "  populer             : 0.268240\n",
            "  bahasa              : 0.085138\n",
            "  learning            : 0.085138\n",
            "  machine             : 0.085138\n",
            "  pemrograman         : 0.085138\n",
            "  python              : 0.085138\n",
            "\n",
            "Dokumen 2: machine learning membutuhkan data banyak pelatihan model \n",
            "----------------------------------------------------------------------\n",
            "Kata (TF-IDF > 0):\n",
            "\n",
            "Kata                  TF-IDF\n",
            "-----------------------------------\n",
            "  banyak              : 0.229920\n",
            "  membutuhkan         : 0.229920\n",
            "  pelatihan           : 0.229920\n",
            "  model               : 0.130899\n",
            "  data                : 0.072975\n",
            "  learning            : 0.072975\n",
            "  machine             : 0.072975\n",
            "\n",
            "Dokumen 3: bahasa pemrograman python mudah dipelajari pemula \n",
            "----------------------------------------------------------------------\n",
            "Kata (TF-IDF > 0):\n",
            "\n",
            "Kata                  TF-IDF\n",
            "-----------------------------------\n",
            "  dipelajari          : 0.268240\n",
            "  mudah               : 0.268240\n",
            "  pemula              : 0.268240\n",
            "  bahasa              : 0.085138\n",
            "  pemrograman         : 0.085138\n",
            "  python              : 0.085138\n",
            "\n",
            "Dokumen 4: data science menggunakan python bahasa pemrograman utama \n",
            "----------------------------------------------------------------------\n",
            "Kata (TF-IDF > 0):\n",
            "\n",
            "Kata                  TF-IDF\n",
            "-----------------------------------\n",
            "  menggunakan         : 0.229920\n",
            "  science             : 0.229920\n",
            "  utama               : 0.229920\n",
            "  bahasa              : 0.072975\n",
            "  data                : 0.072975\n",
            "  pemrograman         : 0.072975\n",
            "  python              : 0.072975\n",
            "\n",
            "Dokumen 5: model machine learning digunakan prediksi data \n",
            "----------------------------------------------------------------------\n",
            "Kata (TF-IDF > 0):\n",
            "\n",
            "Kata                  TF-IDF\n",
            "-----------------------------------\n",
            "  digunakan           : 0.268240\n",
            "  prediksi            : 0.268240\n",
            "  model               : 0.152715\n",
            "  data                : 0.085138\n",
            "  learning            : 0.085138\n",
            "  machine             : 0.085138\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(tfidf_vectors)):\n",
        "    tfidf_vec = tfidf_vectors[i]\n",
        "    doc_tokens = processed_docs[i]\n",
        "\n",
        "    # Gabungkan tokens menjadi string\n",
        "    doc_str = \"\"\n",
        "    for token in doc_tokens:\n",
        "        doc_str += token + \" \"\n",
        "\n",
        "    print(f\"\\nDokumen {i + 1}: {doc_str}\")\n",
        "    cetak_garis(70, \"-\")\n",
        "\n",
        "    # Urutkan kata berdasarkan nilai TF-IDF\n",
        "    sorted_tfidf = tfidf_extractor.urutkan_tfidf(tfidf_vec)\n",
        "\n",
        "    # Tampilkan kata dengan TF-IDF > 0\n",
        "    print(\"Kata (TF-IDF > 0):\")\n",
        "    print(\"\\nKata                  TF-IDF\")\n",
        "    print(\"-\" * 35)\n",
        "    count = 0\n",
        "    for item in sorted_tfidf:\n",
        "        word = item[0]\n",
        "        score = item[1]\n",
        "        if score > 0:\n",
        "            padding = \" \" * (20 - len(word))\n",
        "            print(f\"  {word}{padding}: {format_angka(score, 6)}\")\n",
        "            count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        print(\"  (Tidak ada kata dengan TF-IDF > 0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section10"
      },
      "source": [
        "## 10. Representasi Matriks TF-IDF\n",
        "\n",
        "Tampilkan TF-IDF dalam bentuk matriks (dokumen × fitur)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "matrix_representation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0a3a2c-34f5-43e9-9142-bc5ae1dd2071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimensi: 5 dokumen × 19 fitur\n",
            "\n",
            "Menampilkan 5 fitur pertama:\n",
            "\n",
            "Dok    bahasa    banyak     data   digunakan dipelajari...\n",
            "----------------------------------------------------------\n",
            " 1     0.0851    0.0000    0.0000    0.0000    0.0000  ...\n",
            " 2     0.0000    0.2299    0.0730    0.0000    0.0000  ...\n",
            " 3     0.0851    0.0000    0.0000    0.0000    0.2682  ...\n",
            " 4     0.0730    0.0000    0.0730    0.0000    0.0000  ...\n",
            " 5     0.0000    0.0000    0.0851    0.2682    0.0000  ...\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nDimensi: {len(tfidf_vectors)} dokumen × {len(vocabulary)} fitur\\n\")\n",
        "\n",
        "# Tampilkan 5 fitur pertama sebagai contoh\n",
        "num_features_to_show = min(5, len(vocabulary))\n",
        "print(f\"Menampilkan {num_features_to_show} fitur pertama:\\n\")\n",
        "\n",
        "# Header\n",
        "print(\"Dok  \", end=\"\")\n",
        "for j in range(num_features_to_show):\n",
        "    word = vocabulary[j]\n",
        "    padding_left = (10 - len(word)) // 2\n",
        "    padding_right = 10 - len(word) - padding_left\n",
        "    print(\" \" * padding_left + word + \" \" * padding_right, end=\"\")\n",
        "print(\"...\")\n",
        "\n",
        "# Separator\n",
        "print(\"-\" * (5 + num_features_to_show * 10 + 3))\n",
        "\n",
        "# Data\n",
        "for i in range(len(tfidf_vectors)):\n",
        "    tfidf_vec = tfidf_vectors[i]\n",
        "    print(f\" {i + 1}   \", end=\"\")\n",
        "    vec_list = tfidf_extractor.vector_to_list(tfidf_vec)\n",
        "    for j in range(num_features_to_show):\n",
        "        val = vec_list[j]\n",
        "        val_str = format_angka(val, 4)\n",
        "        padding_left = (10 - len(val_str)) // 2\n",
        "        padding_right = 10 - len(val_str) - padding_left\n",
        "        print(\" \" * padding_left + val_str + \" \" * padding_right, end=\"\")\n",
        "    print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section11"
      },
      "source": [
        "## 11. Statistik dan Analisis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "statistics",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d8ae14-4a7d-4f4e-ac70-31e1e5d2d094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Statistik Umum:\n",
            "  • Jumlah dokumen        : 5\n",
            "  • Total token (setelah preprocessing): 32\n",
            "  • Rata-rata token per dokumen: 6.40\n",
            "  • Jumlah fitur unik     : 19\n",
            "\n",
            "  Kata dengan IDF tertinggi:\n",
            "  • 'banyak' dengan IDF = 1.6094\n",
            "\n",
            "  Kata dengan IDF terendah:\n",
            "  • 'bahasa' dengan IDF = 0.5108\n"
          ]
        }
      ],
      "source": [
        "# Hitung statistik\n",
        "total_tokens = 0\n",
        "for doc in processed_docs:\n",
        "    total_tokens += len(doc)\n",
        "\n",
        "avg_tokens = total_tokens / len(processed_docs) if len(processed_docs) > 0 else 0\n",
        "\n",
        "print(f\"  Statistik Umum:\")\n",
        "print(f\"  • Jumlah dokumen        : {len(documents_text)}\")\n",
        "print(f\"  • Total token (setelah preprocessing): {total_tokens}\")\n",
        "print(f\"  • Rata-rata token per dokumen: {format_angka(avg_tokens, 2)}\")\n",
        "print(f\"  • Jumlah fitur unik     : {len(vocabulary)}\")\n",
        "\n",
        "# Cari kata dengan IDF tertinggi dan terendah\n",
        "max_idf = -999999\n",
        "min_idf = 999999\n",
        "max_word = \"\"\n",
        "min_word = \"\"\n",
        "\n",
        "for word in vocabulary:\n",
        "    idf = tfidf_extractor.idf_values[word]\n",
        "    if idf > max_idf:\n",
        "        max_idf = idf\n",
        "        max_word = word\n",
        "    if idf < min_idf:\n",
        "        min_idf = idf\n",
        "        min_word = word\n",
        "\n",
        "print(f\"\\n  Kata dengan IDF tertinggi:\")\n",
        "print(f\"  • '{max_word}' dengan IDF = {format_angka(max_idf, 4)}\")\n",
        "\n",
        "print(f\"\\n  Kata dengan IDF terendah:\")\n",
        "print(f\"  • '{min_word}' dengan IDF = {format_angka(min_idf, 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section12"
      },
      "source": [
        "## 12. Menggunakan dokumen lain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbce7fcb-c854-4fdd-ba37-aea2c0c736e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Preprocessing:\n",
            "  Dok 1: ['artificial', 'intelligence', 'mengubah', 'cara', 'kita', 'bekerja']\n",
            "  Dok 2: ['deep', 'learning', 'subset', 'machine', 'learning']\n",
            "  Dok 3: ['natural', 'language', 'processing', 'membantu', 'komputer', 'memahami', 'bahasa', 'manusia']\n",
            "\n",
            "2. Vocabulary:\n",
            "  ['artificial', 'bahasa', 'bekerja', 'cara', 'deep', 'intelligence', 'kita', 'komputer', 'language', 'learning', 'machine', 'manusia', 'memahami', 'membantu', 'mengubah', 'natural', 'processing', 'subset']\n",
            "\n",
            "3. Top kata per dokumen:\n",
            "\n",
            "  Dokumen 1:\n",
            "    - artificial: 0.1831\n",
            "    - bekerja: 0.1831\n",
            "    - cara: 0.1831\n",
            "\n",
            "  Dokumen 2:\n",
            "    - learning: 0.4394\n",
            "    - deep: 0.2197\n",
            "    - machine: 0.2197\n",
            "\n",
            "  Dokumen 3:\n",
            "    - bahasa: 0.1373\n",
            "    - komputer: 0.1373\n",
            "    - language: 0.1373\n"
          ]
        }
      ],
      "source": [
        "# Ganti dengan dokumen lain\n",
        "dokumen_baru = [\n",
        "    \"Artificial intelligence mengubah cara kita bekerja\",\n",
        "    \"Deep learning adalah subset dari machine learning\",\n",
        "    \"Natural language processing membantu komputer memahami bahasa manusia\"\n",
        "]\n",
        "\n",
        "# Preprocessing\n",
        "preprocessor_baru = TextPreprocessor()\n",
        "docs_terproses = []\n",
        "\n",
        "print(\"\\n1. Preprocessing:\")\n",
        "for i, doc in enumerate(dokumen_baru, 1):\n",
        "    tokens = preprocessor_baru.preprocess(doc)\n",
        "    docs_terproses.append(tokens)\n",
        "    print(f\"  Dok {i}: {tokens}\")\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_baru = TFIDFExtractor()\n",
        "vectors_baru = tfidf_baru.fit_transform(docs_terproses)\n",
        "\n",
        "print(\"\\n2. Vocabulary:\")\n",
        "print(f\"  {tfidf_baru.get_feature_names()}\")\n",
        "\n",
        "print(\"\\n3. Top kata per dokumen:\")\n",
        "for i, vec in enumerate(vectors_baru, 1):\n",
        "    sorted_vec = tfidf_baru.urutkan_tfidf(vec)\n",
        "    print(f\"\\n  Dokumen {i}:\")\n",
        "    for word, score in sorted_vec[:3]:  # Top 3\n",
        "        if score > 0:\n",
        "            print(f\"    - {word}: {format_angka(score, 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Kesimpulan\n",
        "\n",
        "### Yang Telah Dipelajari:\n",
        "1. Preprocessing teks (lowercase, remove punctuation, tokenization, stopwords removal)\n",
        "2. Perhitungan Term Frequency (TF)\n",
        "3. Perhitungan Inverse Document Frequency (IDF)\n",
        "4. Perhitungan TF-IDF\n",
        "5. Representasi dokumen dalam bentuk vektor numerik\n",
        "\n",
        "### Keuntungan Membuat from Scratch Tanpa Library:\n",
        "- Memahami konsep fundamental\n",
        "- Dapat melakukan customisasi sesuai kebutuhan\n",
        "- Dasar yang kuat untuk mempelajari teknik NLP lanjutan"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}